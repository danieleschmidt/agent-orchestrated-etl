# ğŸ¤– Agent-Orchestrated-ETL

> **Intelligent, Self-Optimizing Data Pipelines Powered by AI Agents**

A revolutionary hybrid system combining Apache Airflow's robust workflow orchestration with LangChain's intelligent agent capabilities to create adaptive, self-healing data pipelines that learn and optimize automatically.

[![Build Status](https://github.com/danieleschmidt/agent-orchestrated-etl/workflows/CI/badge.svg)](https://github.com/danieleschmidt/agent-orchestrated-etl/actions)
[![Coverage](https://codecov.io/gh/danieleschmidt/agent-orchestrated-etl/branch/main/graph/badge.svg)](https://codecov.io/gh/danieleschmidt/agent-orchestrated-etl)
[![Documentation](https://img.shields.io/badge/docs-latest-blue.svg)](https://danieleschmidt.github.io/agent-orchestrated-etl/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

## ğŸ“‹ Table of Contents

- [Key Features](#-key-features)
- [Quick Start](#-quick-start)
- [Architecture Overview](#ï¸-architecture-overview)
- [Usage Examples](#-usage-examples)
- [CLI Commands](#ï¸-cli-commands)
- [Development Setup](#-development-setup)
- [Contributing](#-contributing)
- [Documentation](#-documentation)
- [License](#-license)

## âœ¨ Key Features

ğŸ§  **AI-Driven Pipeline Generation** - Automatically analyzes data sources and generates optimal ETL workflows  
ğŸ”„ **Self-Healing Pipelines** - Agents detect failures and implement recovery strategies autonomously  
âš¡ **Dynamic Optimization** - Real-time performance tuning based on execution patterns  
ğŸ”Œ **Universal Connectors** - Native support for S3, PostgreSQL, APIs, files, and more  
ğŸ“Š **Intelligent Monitoring** - Proactive issue detection with automated resolution suggestions  
ğŸš€ **Production Ready** - Enterprise-grade security, scaling, and compliance features  
ğŸ¯ **Autonomous SDLC** - Self-improving system with progressive enhancement generations  
âš¡ **Advanced Scaling** - Multi-level caching, load balancing, and predictive auto-scaling  
ğŸ›¡ï¸ **Comprehensive Security** - Multi-layer validation, encryption, and threat detection  
ğŸ”¬ **Research-Ready** - Built-in frameworks for ML experimentation and benchmarking

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Docker & Docker Compose (optional, for containerized setup)

### Installation

```bash
# Clone the repository
git clone https://github.com/danieleschmidt/agent-orchestrated-etl.git
cd agent-orchestrated-etl

# Install dependencies
pip install -r requirements.txt

# Setup Airflow
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init

# Initialize agent system
python setup_agent.py

# Start the system
airflow webserver --port 8080
```

### Docker Setup (Recommended)

```bash
# Start all services
docker-compose up -d

# Access Airflow UI
open http://localhost:8080
```

## ğŸ—ï¸ Architecture Overview

Our intelligent agent architecture consists of three core components:

- **ğŸ¯ Orchestrator Agent**: Analyzes data sources and orchestrates optimal pipeline creation
- **âš™ï¸ ETL Agents**: Specialized agents handling extraction, transformation, and loading operations  
- **ğŸ‘ï¸ Monitor Agent**: Continuously monitors pipeline health and suggests performance optimizations

[ğŸ“– Detailed Architecture Documentation](docs/architecture/system-overview.md)

## ğŸ’¡ Usage Examples
```python
from agent_orchestrated_etl import DataOrchestrator

orch = DataOrchestrator()
pipeline = orch.create_pipeline(
    source="s3://my-bucket/data/",
)
pipeline.execute()

# override the load step
custom_pipeline = orch.create_pipeline(
    source="s3://my-bucket/data/",
    operations={"load": lambda data: print("loaded", data)},
)
custom_pipeline.execute()

# create a pipeline from a REST API
api_pipeline = orch.create_pipeline(
    source="api://example.com/endpoint",
)
api_pipeline.execute()
```

## ğŸ–¥ï¸ CLI Commands

Execute pipelines directly from the command line with powerful options:

```bash
# Run pipeline with monitoring
run_pipeline s3 --output results.json --monitor events.log

# Preview pipeline without execution
run_pipeline s3 --list-tasks

# Generate Airflow DAG
generate_dag s3 dag.py --dag-id my_dag

# List available data sources
run_pipeline --list-sources
```

**CLI Options:**
- `--list-tasks`: Preview execution order without running
- `--list-sources`: Show supported data sources  
- `--monitor <file>`: Capture task events to file
- `--output <file>`: Specify output location
- `--dag-id <id>`: Set custom DAG identifier

## ğŸ”§ Development Setup
Install development dependencies and enable pre-commit hooks:

```bash
# Install in development mode with all dependencies
pip install -e .[dev]

# Setup pre-commit hooks for code quality
pre-commit install

# Run tests with coverage
pytest -q
coverage run -m pytest -q
coverage report -m

# Check code complexity
radon cc src/agent_orchestrated_etl -s -a

# Run linting and formatting
black . && isort . && flake8
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Quick Contributing Steps
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“š Documentation

- [ğŸ“– Full Documentation](docs/)
- [ğŸ—ï¸ Architecture Guide](docs/architecture/system-overview.md)
- [ğŸš€ Quick Start Guide](docs/guides/quick-start.md)
- [ğŸ”§ Development Guide](docs/developer/onboarding.md)
- [ğŸ“‹ API Reference](docs/api/api-reference.md)
- [ğŸ—ºï¸ Project Roadmap](docs/ROADMAP.md)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**[â­ Star us on GitHub](https://github.com/danieleschmidt/agent-orchestrated-etl)** | **[ğŸ“ Report Issues](https://github.com/danieleschmidt/agent-orchestrated-etl/issues)** | **[ğŸ’¬ Join Discussions](https://github.com/danieleschmidt/agent-orchestrated-etl/discussions)**

Made with â¤ï¸ by the Agent-Orchestrated-ETL team

</div>
