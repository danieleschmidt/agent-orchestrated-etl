# Production override for Agent-Orchestrated-ETL
# Usage: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
version: '3.8'

services:
  # =============================================================================
  # Production Application Configuration
  # =============================================================================
  
  app:
    build:
      target: production
      args:
        BUILD_DATE: ${BUILD_DATE}
        VCS_REF: ${VCS_REF}
        VERSION: ${VERSION}
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=INFO
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW_DATABASE_URL}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
    volumes:
      - app_logs:/app/logs
      - app_data:/app/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "python", "-c", "import agent_orchestrated_etl; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: ["python", "-m", "agent_orchestrated_etl.cli", "run-server"]

  # =============================================================================
  # Production Database Configuration
  # =============================================================================
  
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_prod_data:/var/lib/postgresql/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c pg_stat_statements.track=all
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c work_mem=4MB
      -c maintenance_work_mem=64MB
      -c random_page_cost=1.1
      -c temp_file_limit=2GB
      -c log_min_duration_statement=1000
      -c log_statement=ddl
      -c log_checkpoints=on
      -c log_connections=on
      -c log_disconnections=on
      -c log_lock_waits=on

  # =============================================================================
  # Production Redis Configuration
  # =============================================================================
  
  redis:
    image: redis:7-alpine
    volumes:
      - redis_prod_data:/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.1'
          memory: 256M
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
      --rdbcompression yes
      --rdbchecksum yes

  # =============================================================================
  # Production Airflow Configuration
  # =============================================================================
  
  airflow-webserver:
    build:
      target: production
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW_DATABASE_URL}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__WEBSERVER__WORKERS=4
      - AIRFLOW__WEBSERVER__WORKER_TIMEOUT=120
      - AIRFLOW__WEBSERVER__WEB_SERVER_WORKER_TIMEOUT=120
    volumes:
      - airflow_logs:/app/logs
      - airflow_dags:/app/dags:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  airflow-scheduler:
    build:
      target: production
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW_DATABASE_URL}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__SCHEDULER__NUM_RUNS=-1
      - AIRFLOW__SCHEDULER__PROCESSOR_POLL_INTERVAL=1
    volumes:
      - airflow_logs:/app/logs
      - airflow_dags:/app/dags:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  # =============================================================================
  # Production Worker Configuration
  # =============================================================================
  
  airflow-worker:
    build:
      target: production
    environment:
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW_DATABASE_URL}
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CELERY__BROKER_URL=${REDIS_URL}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+${AIRFLOW_DATABASE_URL}
    volumes:
      - airflow_logs:/app/logs
      - airflow_dags:/app/dags:ro
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 1G
    command: ["airflow", "celery", "worker"]

  # =============================================================================
  # Production Monitoring
  # =============================================================================
  
  prometheus:
    volumes:
      - ./monitoring/prometheus.prod.yml:/etc/prometheus/prometheus.yml
      - prometheus_prod_data:/prometheus
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

  grafana:
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
    volumes:
      - grafana_prod_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # =============================================================================
  # Production Reverse Proxy
  # =============================================================================
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    restart: unless-stopped
    depends_on:
      - app
      - airflow-webserver
      - grafana
    networks:
      - agent-etl-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

  # =============================================================================
  # Log Aggregation
  # =============================================================================
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    networks:
      - agent-etl-network

  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.0
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - app_logs:/app/logs:ro
      - airflow_logs:/airflow/logs:ro
      - nginx_logs:/nginx/logs:ro
    depends_on:
      - elasticsearch
    restart: unless-stopped
    networks:
      - agent-etl-network

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    restart: unless-stopped
    networks:
      - agent-etl-network

# =============================================================================
# Production Volumes
# =============================================================================

volumes:
  postgres_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/data/postgres
  redis_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/data/redis
  prometheus_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/data/prometheus
  grafana_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/data/grafana
  app_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/logs/app
  airflow_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/logs/airflow
  airflow_dags:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/dags
  nginx_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/logs/nginx
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/agent-etl/data/elasticsearch